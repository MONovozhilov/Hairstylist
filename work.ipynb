{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4bdc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to results/portrait_matte.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"MODNet/src\")\n",
    "\n",
    "\n",
    "from models.modnet import MODNet\n",
    "\n",
    "class HairSegmenter:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
    "        self.model = MODNet(backbone_pretrained=False)\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤\n",
    "        ckpt_path = \"MODNet/pretrained/modnet_photographic_portrait_matting.ckpt\"\n",
    "        weights = torch.load(ckpt_path, map_location=self.device)\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        self.orig_size = image.size\n",
    "        \n",
    "        im = np.asarray(image).astype(np.float32)\n",
    "        im = cv2.resize(im, (512, 512))\n",
    "        \n",
    "        im = im / 255.0\n",
    "        im = (im - 0.5) / 0.5\n",
    "        \n",
    "        im = torch.from_numpy(im).permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        return im.to(self.device)\n",
    "\n",
    "    def predict(self, image_path, output_path='results/output.png'):\n",
    "        \n",
    "        input_tensor = self.preprocess_image(image_path)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, _, matte = self.model(input_tensor, True)\n",
    "        \n",
    "        matte = matte[0][0].data.cpu().numpy()\n",
    "        matte = (matte * 255).astype(np.uint8)\n",
    "        \n",
    "        matte_pil = Image.fromarray(matte)\n",
    "        matte_pil = matte_pil.resize(self.orig_size, Image.BILINEAR)\n",
    "        \n",
    "        original = Image.open(image_path).convert('RGBA')\n",
    "        original.putalpha(matte_pil)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        original.save(output_path)\n",
    "        \n",
    "        print(f\"Result saved to {output_path}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    segmenter = HairSegmenter()\n",
    "    if os.path.exists('data/input.jpeg'):\n",
    "        segmenter.predict('data/input.jpeg', 'results/portrait_matte.png')\n",
    "    else:\n",
    "        print(\"Please put an image at data/input.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb965f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hair mask saved to results/hair_only.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def refine_hair_mask(portrait_image_path, matte_image_path, output_path='results/hair_only.png'):\n",
    "\n",
    "    \n",
    "    # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ PIL (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π) ===\n",
    "    try:\n",
    "        original_pil = Image.open(portrait_image_path).convert('RGB')\n",
    "        matte_pil = Image.open(matte_image_path).convert('RGBA')\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load images: {e}\")\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PIL -> OpenCV (BGR)\n",
    "    original = cv2.cvtColor(np.array(original_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # –î–ª—è –º–∞—Ç—Ç—ã –±–µ—Ä–µ–º –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª\n",
    "    matte_rgba = np.array(matte_pil)\n",
    "    if matte_rgba.shape[2] == 4:\n",
    "        matte = matte_rgba[:, :, 3]  # –ê–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª\n",
    "    else:\n",
    "        matte = cv2.cvtColor(matte_rgba, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # === 1. –ë–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Å–∫–∞ ===\n",
    "    _, binary_mask = cv2.threshold(matte, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # === 2. –ö–æ–Ω—Ç—É—Ä –≥–æ–ª–æ–≤—ã ===\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        h, w = matte.shape\n",
    "        x, y, w, h = int(w*0.25), int(h*0.1), int(w*0.5), int(h*0.6)\n",
    "    else:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "    \n",
    "    # === 3. –ó–æ–Ω–∞ –≤–æ–ª–æ—Å (–≤–µ—Ä—Ö–Ω—è—è —á–∞—Å—Ç—å) ===\n",
    "    hair_roi_y = int(y) \n",
    "    hair_roi_h = int(h * 0.45)\n",
    "    \n",
    "    hair_zone_mask = np.zeros_like(matte)\n",
    "    cv2.rectangle(hair_zone_mask, (x, hair_roi_y), (x + w, hair_roi_y + hair_roi_h), 255, -1)\n",
    "    \n",
    "    # === 4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Å–æ–∫ ===\n",
    "    hair_mask = cv2.bitwise_and(matte, matte, mask=hair_zone_mask)\n",
    "    \n",
    "    # === 5. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è ===\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    dilated_mask = cv2.dilate(hair_mask, kernel, iterations=2)\n",
    "    eroded_mask = cv2.erode(dilated_mask, kernel, iterations=1)\n",
    "    \n",
    "    # === 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ===\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º RGBA –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    b, g, r = cv2.split(original)\n",
    "    rgba = [b, g, r, eroded_mask]\n",
    "    dst = cv2.merge(rgba, 4)\n",
    "    \n",
    "    cv2.imwrite(output_path, dst)\n",
    "    print(f\"‚úÖ Hair mask saved to {output_path}\")\n",
    "    \n",
    "    # === –û—Ç–ª–∞–¥–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã ===\n",
    "    cv2.imwrite('results/debug_binary_mask.png', binary_mask)\n",
    "    cv2.imwrite('results/debug_hair_zone.png', hair_zone_mask)\n",
    "    cv2.imwrite('results/debug_final_mask.png', eroded_mask)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    refine_hair_mask('data/input.jpeg', 'results/portrait_matte.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3007ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Failed to load image: module 'cv2' has no attribute 'COLOR_RGB2RGB'\n",
      "‚úÖ Hair mask saved to results/hair_only.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class FaceLandmarkDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=True,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=False,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # –ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –¥–ª—è –≤–æ–ª–æ—Å (MediaPipe Face Mesh)\n",
    "        self.hairline_points = [\n",
    "            10, 151, 162, 163, 164, 165, 166, 167,  # –õ–∏–Ω–∏—è –≤–æ–ª–æ—Å —Å–ø–µ—Ä–µ–¥–∏\n",
    "            12, 13, 14, 15, 16, 17,  # –õ–æ–±\n",
    "        ]\n",
    "        \n",
    "    def get_hair_region(self, image_path):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç bounding box –∑–æ–Ω—ã –≤–æ–ª–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª—ç–Ω–¥–º–∞—Ä–∫–æ–≤.\n",
    "        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ PIL –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –ø—É—Ç–µ–π.\n",
    "        \"\"\"\n",
    "        # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ PIL ===\n",
    "        try:\n",
    "            image_pil = Image.open(image_path).convert('RGB')\n",
    "            image_np = np.array(image_pil)\n",
    "            image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2RGB)  # PIL —É–∂–µ RGB\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Failed to load image: {e}\")\n",
    "            return None\n",
    "        \n",
    "        height, width = image_rgb.shape[:2]\n",
    "        \n",
    "        results = self.face_mesh.process(image_rgb)\n",
    "        \n",
    "        if not results.multi_face_landmarks:\n",
    "            print(\"‚ö†Ô∏è  No face detected! Using fallback heuristic.\")\n",
    "            return None\n",
    "        \n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –≤–µ—Ä—Ö–Ω—é—é —Ç–æ—á–∫—É –ª–±–∞/–ª–∏–Ω–∏–∏ –≤–æ–ª–æ—Å\n",
    "        top_y = height\n",
    "        left_x = width\n",
    "        right_x = 0\n",
    "        \n",
    "        for idx in self.hairline_points:\n",
    "            if idx < len(landmarks):\n",
    "                point = landmarks[idx]\n",
    "                px = int(point.x * width)\n",
    "                py = int(point.y * height)\n",
    "                \n",
    "                top_y = min(top_y, py)\n",
    "                left_x = min(left_x, px)\n",
    "                right_x = max(right_x, px)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º padding –≤–≤–µ—Ä—Ö –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ø—Ä–∏—á–µ—Å–∫–∏\n",
    "        padding_top = int(height * 0.15)\n",
    "        padding_sides = int(width * 0.1)\n",
    "        \n",
    "        hair_bbox = {\n",
    "            'x': max(0, left_x - padding_sides),\n",
    "            'y': max(0, top_y - padding_top),\n",
    "            'w': min(width, right_x + padding_sides) - max(0, left_x - padding_sides),\n",
    "            'h': max(0, top_y - padding_top) + int(height * 0.25)\n",
    "        }\n",
    "        \n",
    "        print(f\"üìç Hair region from landmarks: {hair_bbox}\")\n",
    "        return hair_bbox\n",
    "    \n",
    "    def close(self):\n",
    "        self.face_mesh.close()\n",
    "\n",
    "\n",
    "def refine_hair_mask_with_landmarks(portrait_image_path, matte_image_path, output_path='results/hair_only.png'):\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MediaPipe Face Mesh.\n",
    "    \"\"\"\n",
    "    # === –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—á–µ—Ä–µ–∑ PIL –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã) ===\n",
    "    original_pil = Image.open(portrait_image_path).convert('RGB')\n",
    "    matte_pil = Image.open(matte_image_path).convert('RGBA')\n",
    "    \n",
    "    original = cv2.cvtColor(np.array(original_pil), cv2.COLOR_RGB2BGR)\n",
    "    matte_rgba = np.array(matte_pil)\n",
    "    \n",
    "    if matte_rgba.shape[2] == 4:\n",
    "        matte = matte_rgba[:, :, 3]\n",
    "    else:\n",
    "        matte = cv2.cvtColor(matte_rgba, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    height, width = matte.shape\n",
    "    \n",
    "    # === –ü–æ–ª—É—á–∞–µ–º –∑–æ–Ω—É –≤–æ–ª–æ—Å –∏–∑ –ª—ç–Ω–¥–º–∞—Ä–∫–æ–≤ ===\n",
    "    detector = FaceLandmarkDetector()\n",
    "    hair_bbox = detector.get_hair_region(portrait_image_path)\n",
    "    detector.close()\n",
    "    \n",
    "    # === Fallback –µ—Å–ª–∏ –ª–∏—Ü–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ===\n",
    "    if hair_bbox is None:\n",
    "        contours, _ = cv2.findContours(\n",
    "            cv2.threshold(matte, 25, 255, cv2.THRESH_BINARY)[1], \n",
    "            cv2.RETR_EXTERNAL, \n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        if contours:\n",
    "            c = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            hair_bbox = {\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'w': w,\n",
    "                'h': int(h * 0.25)\n",
    "            }\n",
    "    \n",
    "    # === –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –∑–æ–Ω—ã –≤–æ–ª–æ—Å ===\n",
    "    hair_zone_mask = np.zeros_like(matte)\n",
    "    cv2.rectangle(\n",
    "        hair_zone_mask,\n",
    "        (hair_bbox['x'], hair_bbox['y']),\n",
    "        (hair_bbox['x'] + hair_bbox['w'], hair_bbox['y'] + hair_bbox['h']),\n",
    "        255,\n",
    "        -1\n",
    "    )\n",
    "    \n",
    "    # === –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –º–∞—Ç—Ç–æ–π ===\n",
    "    hair_mask = cv2.bitwise_and(matte, matte, mask=hair_zone_mask)\n",
    "    \n",
    "    # === –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è ===\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    dilated_mask = cv2.dilate(hair_mask, kernel, iterations=2)\n",
    "    eroded_mask = cv2.erode(dilated_mask, kernel, iterations=1)\n",
    "    \n",
    "    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    b, g, r = cv2.split(original)\n",
    "    rgba = [b, g, r, eroded_mask]\n",
    "    dst = cv2.merge(rgba, 4)\n",
    "    \n",
    "    cv2.imwrite(output_path, dst)\n",
    "    cv2.imwrite('results/debug_hair_zone_landmarks.png', hair_zone_mask)\n",
    "    cv2.imwrite('results/debug_final_mask_landmarks.png', eroded_mask)\n",
    "    \n",
    "    print(f\"‚úÖ Hair mask saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    refine_hair_mask_with_landmarks('data/input.jpeg', 'results/portrait_matte.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
